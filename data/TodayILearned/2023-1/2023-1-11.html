<pre>
    박조은 강사님의 데이터 사이언스 강의

    이전 강의 요약


    이번주 강의 요약


    이번주 실습 요약
        0101
            데이터 수집을 위한 판다스 기초를 배웠습니다.

        0102
            추상화된 라이브리러인 FinanceDataReader 를 통해서 KRX 에 상장된 종목 정보를 수집했습니다.

        0103
            네이버 금융의 뉴스기사를 수집했습니다.
            브라우저의 네트워크 탭을 통해 데이터를 주고 받는 URL 을 확인하는 방법을 배웠습니다.
            f-string 으로 URL을 만들어 주었습니다.
            pd.read_html() 만으로도 데이터 수집이 가능했습니다.
            for 문을 사용해서 여러 페이지를 수집해 봤습니다.

        0104
            네이버 금융의 특정 종목의 일별시세를 수집했습니다.
            사이트에서 user-agent 가 브라우저가 아니면 응답을 주지 않습니다.
            requests의 get을 사용해서 headers를 지정해서 데이터를 수집해 왔습니다.
            BeautifulSoup 을 사용하여 원하는 태그를 찾는 방법을 알아봤습니다.
            read_html 에 html 소스코드를 텍스트 형태로 넣어서 테이블 태그만 찾아왔습니다.
            while 반복문, if 조건문을 사용해서 특정 날짜까지 데이터를 수집했습니다.
            중복데이터 제거, 컬럼명변경, 파생변수 생성, 마지막 거래일로 파일명 만들기
            csv 파일로 저장하고 불러오기를 했습니다.
        0105
            1. 수집하고자 하는 대상의 URL 을 알아봅니다.
            2. 사이트에 접속한다. => HTTP 요청을 보냅니다. requests
            3. 목록을 받아온다. (번호,    대통령,    형태,    유형,    제목,    연설일자) + BeautifulSoup을 통해 내용의 링크 주소도 함께 수집 
            4. 3번을 반복해서 전체 데이터를 수집한다.
            5. 파일로 저장한다.
        
        0106
            1. 0105에서 저장한 대통령 연설문 목록을 가져옵니다.
            2. 내용링크를 통해 각 연설문의 내용 링크에 요청을 보냅니다.
            3. BeautifulSoup을 통해 내용을 찾아옵니다.
            4. 내용의 링크를 통해 내용을 찾는 함수를 만듭니다.
            5. 전체 게시물에 내용을 찾는 함수를 적용합니다. => 반복문을 사용하지 않고 Pandas 의 map, apply 등의 기능을 사용할 예정입니다.
            6. 수집이 되면 내용 파생변수를 생성해 줍니다.
            7. 저장합니다.
       

    새로 접한 내용

</pre>